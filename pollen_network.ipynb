{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollen-Detection-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a class for fully convolutional neural networks.\n",
    "    \n",
    "    It is a subclass of the Module class from torch.nn.\n",
    "    See the torch.nn documentation for more information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor for FCNN class. The internal states of the network are initialized. \n",
    "        \"\"\"\n",
    "        \n",
    "        super(FCNN, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=1, out_channels=30, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=5, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, stride=1)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=60, out_channels=60, kernel_size=3, stride=2)\n",
    "        self.conv3_drop = nn.Dropout2d()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=60, out_channels=120, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=120, out_channels=120, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=120, out_channels=1, kernel_size=1, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        \n",
    "        Parameters:\n",
    "            data (4D-tensor): The input that is evaluated by the network.\n",
    "            \n",
    "        Returns:\n",
    "            x (tensor): The output of the network after evaluating it on the given input.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.conv0(data))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2_drop(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_drop(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = torch.sigmoid(self.conv6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network, testloader):\n",
    "    \"\"\"\n",
    "    Calculates the mean validation loss, the F1-Score and the accuracy of the given network on the testloader data.\n",
    "    \n",
    "    Parameters:\n",
    "        network (): The network to validate. \n",
    "        testloader (torch.utils.data.DataLoader): Contains the data which is used to validate the network. \n",
    "        \n",
    "    Returns:\n",
    "        validation_loss (): The mean loss per image with regards to a fixed criterion.\n",
    "        F1_Score (): The harmonic mean of the precision and the recall of the given network\n",
    "            on the given validation data.\n",
    "        correct / total (): The percentage of correctly classified samples of the total number of validation samples.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "\n",
    "    # We do not need any gradiants here, since we do not train the network.\n",
    "    # We are only interested in the predictions of the network on the testdata. \n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(testloader):\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = network(torch.transpose(inputs[...,None],1,3)).view(-1)\n",
    "            #print(labels.size(0))\n",
    "            loss = criterion(outputs,labels)\n",
    "            validation_loss += loss.item()\n",
    "            predicted = (outputs >= threshold) # Predicted is a tensor of booleans \n",
    "            total += labels.size(0)\n",
    "            predicted = predicted.view(predicted.size(0)) \n",
    "            #print(labels.size())\n",
    "            #print((predicted == labels).sum().item())\n",
    "            labels = labels == 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true_positive += (predicted & labels).sum().item()\n",
    "            false_negative += (torch.logical_not(predicted) & labels).sum().item()\n",
    "            false_positive += (predicted & torch.logical_not(labels)).sum().item()\n",
    "            true_negative += (torch.logical_not(predicted) & torch.logical_not(labels)).sum().item()\n",
    "            \n",
    "        validation_loss = validation_loss / total \n",
    "        \n",
    "        try: \n",
    "            recall = true_positive / (true_positive + false_negative)\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0\n",
    "        try:\n",
    "            precision = true_positive / (true_positive + false_positive)\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0\n",
    "        try:\n",
    "            F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except ZeroDivisionError:\n",
    "            F1_score = 0\n",
    "            \n",
    "    \n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "    print('F1 Score of the network on the test images: %f' % F1_score)\n",
    "    return validation_loss, F1_score, (correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, trainloader, ep, criterion, optimizer, print_interval):\n",
    "    \"\"\"\n",
    "    Train a neural network.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        network ():\n",
    "        trainloader ():\n",
    "        ep (int): The number of epochs the network will be trained for.\n",
    "        criterion ():\n",
    "        optimizer ():\n",
    "        print_interval (int): The number of batches after which the function prints the loss for one batch.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses ([float]): The mean of the loss for one train sample after every epoch. \n",
    "        validation_losses ([float]): The mean of the loss for one validation sample after every epoch.\n",
    "        F1 ([float]): The F1-Score of the validation data after every epoch.\n",
    "        accuracy ([float]): The accuracy of the network on the validation data after every epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    losses = [] # Mean of loss/image in every epoch of training \n",
    "    validation_losses = []\n",
    "    F1 = []\n",
    "    accuracy = []\n",
    "    validate(network, validloader)\n",
    "    \n",
    "    for epoch in range(ep):\n",
    "\n",
    "        loss_stats = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for i, (inputs, lables) in enumerate(trainloader):\n",
    "            \n",
    "            inputs, lables = inputs.to(device), lables.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print(lables.shape)\n",
    "            \n",
    "            outputs = network(torch.transpose(inputs[...,None],1,3)).view(-1)\n",
    "            #print(outputs.shape)\n",
    "            loss = criterion(outputs, lables)\n",
    "            loss.backward() #propagate the error back through the network\n",
    "            optimizer.step() #adjust the weights of the network depending on the propagated error\n",
    "    \n",
    "            #that's it.\n",
    "            total += lables.shape[0]\n",
    "            \n",
    "            #Some statistics:\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            loss_stats += loss.item()\n",
    "            \n",
    "            if i % print_interval == print_interval - 1:    # print every x mini-batches (loss for one batch)\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / print_interval))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        losses.append(loss_stats / total)\n",
    "        #Test the network using the validation set after every epoch of training.\n",
    "        val_loss_curr, F1_curr, accuracy_curr = validate(network, validloader)\n",
    "        validation_losses.append(val_loss_curr)\n",
    "        F1.append(F1_curr)\n",
    "        accuracy.append(accuracy_curr)\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return losses, validation_losses, F1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pollen_data(dir):\n",
    "    np_pollen_data = []     # single date will be [img,label]\n",
    "    for folder in next(os.walk(dir))[1]:\n",
    "        if folder == 'p':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        parent_path = os.path.join(dir, folder)\n",
    "        for file in os.listdir(parent_path):\n",
    "            if '.png' in file:\n",
    "                try:\n",
    "                    path = os.path.join(parent_path, file)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    np_pollen_data.append([np.array(img), label])\n",
    "                except Exception as e:\n",
    "                    print(folder, file, str(e))\n",
    "\n",
    "    data = np.random.shuffle(np_pollen_data)\n",
    "    print(\"Data created.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_val_sets(data, ratio=0.8):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(np_pollen_data)):\n",
    "        images.append(np_pollen_data[i][0])\n",
    "        labels.append(np_pollen_data[i][1])\n",
    "    \n",
    "    train_x = images[:int(len(images)*0.8)]\n",
    "    train_y = labels[:int(len(images)*0.8)]\n",
    "    valid_x = images[int(len(images)*0.8):]\n",
    "    valid_y = labels[int(len(images)*0.8):]\n",
    "    \n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "cur_dir = os.getcwd()\n",
    "pollen_path = cur_dir + '/Datasets/PollenDataSmall/'\n",
    "pollendata = load_pollen_data(pollen_path)\n",
    "\n",
    "# Split into sets\n",
    "train_x, train_y, valid_x, valid_y = create_test_val_sets(pollendata, 0.8)\n",
    "\n",
    "# Our neural network expects a dataloader. Thus, we \n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(train_x),torch.Tensor(train_y)),batch_size=32)\n",
    "validloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(valid_x),torch.Tensor(valid_y)),batch_size=100)\n",
    "\n",
    "# Save dataloader for later use\n",
    "torch.save(trainloader, 'trainloader.pth')\n",
    "torch.save(validloader, 'validloader.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup wether gpu or cpu should be used for computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Choose optimizer, criterion, number of epochs and threshold for the network to accept or dismiss a computated input\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(pollen_network.parameters(), lr=0.001, momentum=0.9)\n",
    "ep = 10\n",
    "threshold = 0.8\n",
    "\n",
    "# Create a new instance of the FCNN class. This will be our network.      \n",
    "pollen_network = FCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we have a look at the performance of the untrained model on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(network, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can train our network with given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, F1_scores, accuracies = train(network, trainloader, ep, criterion, optimizer, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the networks parameters to have the possibility to reload them later:\n",
    "cur_dir = os.getcwd()\n",
    "save_path = cur_dir + '/pollennet.pt'\n",
    "torch.save(network.state_dict(), save_path)\n",
    "\n",
    "# save the corresponding losses, F1-Scores and Accuracies using pickle:\n",
    "\n",
    "with open('train_losses.obj', 'wb') as train_losses_file:\n",
    "    pickle.dump(train_losses, train_losses_file)\n",
    "\n",
    "with open('val_losses.obj', 'wb') as val_losses_file:\n",
    "    pickle.dump(val_losses, val_losses_file)\n",
    "    \n",
    "with open('F1.obj', 'wb') as F1_file:\n",
    "    pickle.dump(F1_scores, F1_file)\n",
    "    \n",
    "with open('accuracies.obj', 'wb') as accuracies_file:\n",
    "    pickle.dump(accuracies, accuracies_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

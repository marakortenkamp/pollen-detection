{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import errno\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "!pip3 install git+https://github.com/aleju/imgaug\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SuperVisely Object Extractor\n",
    "    For each raw image containing multiple objects markted by Point, Bounding Box or Polygon we create images cropped to given 'size', with each object centered.\n",
    "    We create directories for each class containing those cropped images and a single annotation file in COCO format.\n",
    "    If only one class C exists you can create a 'counterclass' of same size as C, with images randomly cropped from raw images (it is guaranteed those images wont contain objects of C).\n",
    "    You can chose to use 'augmentation' to expand the size of the dataset.\n",
    "    'raw_annotation' describes how the raw data is annotated. So far we can distinguish between supervisely *('supervisely',)* and COCO *(coco, <path>)* annotation.\n",
    "        Note: For COCO annotation the filepath is mandatory. Absolute filepaths must start a leading '/', relative filepaths must not.\n",
    "\n",
    "    Directory Structure:\n",
    "\n",
    "    root/\n",
    "     ├───raw_data_directory/\n",
    "     │\n",
    "     └───Datasets/\n",
    "            ├───<DatasetName>\n",
    "            │       ├───annotation.json\n",
    "           ...      ├───Class1/\n",
    "                    ├───Class2/\n",
    "                   ...\n",
    "                    └───ClassN/\n",
    "                    └───Fake/ (created if 'counterclass' is True)\n",
    "\"\"\"\n",
    "\n",
    "class SVObjectExtractor:\n",
    "    \"\"\"Prepare unprocessed Data\"\"\"\n",
    "\n",
    "    def __init__(self, raw_path, target_path=None, dataset_name=None, object_size=None, resize_to=None, augmentation=False, counterclass=False):\n",
    "        \"\"\"\n",
    "        Keyword arguments:\n",
    "        raw_path        --  path to raw data\n",
    "        target_path     --  path to save dataset at. If an existing dataset shall be expanded, give the location of the <Datasetname> directory.\n",
    "        dataset_name    --  name of the dataset\n",
    "        object_size     --  size of objects in images (single integer. only sqaures generated)\n",
    "        resize_to       --  resize cropped images to resize_to (single integer. only squares generated)\n",
    "        augmentation    --  if True, dataset will 5be expanded by augmentations of the cropped images (default False)\n",
    "        counterclass    --  if True, a 'Fake' class with random images of size 'size' will be created\n",
    "\n",
    "        Class attributes:\n",
    "        RAW_DATA_PATH   --  path to raw data\n",
    "        DATASET_PATH    --  path to dataset directory\n",
    "        DATASET_NAME    --  name of the set to be created\n",
    "        OBJECT_SIZE     --  size of cropped images around segmentation\n",
    "        FINAL_SIZE      --  final size of cropped images\n",
    "        AUGMENTATION    --  boolean whether augmentations of cropped images shall be created or not\n",
    "        COUNTERCLASS    --  boolean, whether a fake class should be created or not\n",
    "        CLASSES         --  list of classes found\n",
    "        DATA_CREATED    --  boolen whether create_data was alrdy called or not\n",
    "        DIRS_MADE       --  boolen whether directories are created or not\n",
    "        DIR_EXISTS      --  boolean whether the Dataset directories already exist or not\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Start initialisation..\")\n",
    "        if os.path.isdir(raw_path):\n",
    "            self.RAW_DATA_PATH = raw_path\n",
    "            invalid_path = False\n",
    "            abort = False\n",
    "        else:\n",
    "            print(\"Error. Given path to raw data does not exist.\")\n",
    "            raise ValueError\n",
    "            invalid_path = True\n",
    "        if not invalid_path:\n",
    "            self.DATASET_NAME = dataset_name\n",
    "            self.DIRS_MADE = False\n",
    "            self.SET_ALRDY_EXISTS = False\n",
    "\n",
    "            # some cases to check, to get the right path\n",
    "            setname_len = len(self.DATASET_NAME)\n",
    "            if target_path:\n",
    "                self.DATASET_PATH = target_path\n",
    "            else:\n",
    "                self.DATASET_PATH = os.getcwd()\n",
    "\n",
    "\n",
    "            if self.DATASET_NAME in self.DATASET_PATH[-(setname_len+1):]:\n",
    "                self.SET_ALRDY_EXISTS = True\n",
    "\n",
    "            elif 'Datasets' in self.DATASET_PATH[-(len('Datasets')+1):]:\n",
    "                if os.path.isdir(os.path.join(self.DATASET_PATH,self.DATASET_NAME)):\n",
    "                    print(\"A directory for given datasetname already exists. The set will be expanded.\")\n",
    "                    if not self.__proceed():\n",
    "                        raise ValueError\n",
    "                        abort = True\n",
    "                    else:\n",
    "                        self.DATASET_PATH = os.path.join(self.DATASET_PATH,self.DATASET_NAME)\n",
    "                        self.SET_ALRDY_EXISTS = True\n",
    "                else:\n",
    "                    self.DATASET_PATH = os.path.join(self.DATASET_PATH, self.DATASET_NAME)\n",
    "            else:\n",
    "                if os.path.isdir(os.path.join(self.DATASET_PATH, 'Datasets', self.DATASET_NAME)):\n",
    "                    print(\"A path to given datasetname already exists. The set will be expanded.\")\n",
    "                    if not self.__proceed():\n",
    "                        raise ValueError\n",
    "                        abort = True\n",
    "                    else:\n",
    "                        self.SET_ALRDY_EXISTS = True\n",
    "                self.DATASET_PATH = os.path.join(self.DATASET_PATH, 'Datasets', self.DATASET_NAME)\n",
    "\n",
    "            if not abort:\n",
    "                self.OBJECT_SIZE = object_size\n",
    "                self.FINAL_SIZE = resize_to\n",
    "                self.AUGMENTATION = augmentation\n",
    "                self.COUNTERCLASS = counterclass\n",
    "                self.CLASSES = self.__getClasses(raw_path)\n",
    "                if self.COUNTERCLASS:\n",
    "                    self.CLASSES.append('Fake')\n",
    "                self.DATA_CREATED = False\n",
    "                self.ANNOTATION = None\n",
    "                self.TOTAL_IMAGES = None\n",
    "                self.TOTAL_ANNOTATIONS = None\n",
    "\n",
    "                print(\"Initialisation complete.\")\n",
    "                dir_ready = self.make_directories()\n",
    "                if dir_ready:\n",
    "                    self.create_data()\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Creates a directory for each class in self.CLASSES\n",
    "    \"\"\"\n",
    "    def make_directories(self):\n",
    "        \"\"\"Create direcotries for each class in self.CLASSES\"\"\"\n",
    "\n",
    "        print(\"Create directories..\")\n",
    "        if self.SET_ALRDY_EXISTS:\n",
    "            if self.DATA_CREATED:\n",
    "                print(\"Everything is already done.\")\n",
    "                return False\n",
    "\n",
    "        elif self.DIRS_MADE:\n",
    "            print(\"Error. Directories were already created, yet could not be found.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            for label in self.CLASSES:\n",
    "                os.makedirs(os.path.join(self.DATASET_PATH, label))\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                print(e)\n",
    "                raise\n",
    "                return False\n",
    "            pass\n",
    "        self.DIRS_MADE = True\n",
    "        print(\"Directories created.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Creates annotationfile and cropped images of raw image.\n",
    "    \"\"\"\n",
    "    def create_data(self):\n",
    "        \"\"\"Creates annotationfile and cropped images of raw image.\"\"\"\n",
    "        print(\"Process Data...\")\n",
    "        # Annotation in COCO format\n",
    "        # either load existing annotation file..\n",
    "        if self.SET_ALRDY_EXISTS:\n",
    "            print(\"Searching for existing annotation file..\")\n",
    "            try:\n",
    "                with open(os.path.join(self.DATASET_PATH, 'annotation.json')) as annotation_file:\n",
    "                    self.ANNOTATION = json.load(annotation_file)\n",
    "                print(\"Loading file succeeded.\")\n",
    "                self.TOTAL_IMAGES = len(self.ANNOTATION['images'])*2\n",
    "                self.TOTAL_ANNOTATIONS = len(self.ANNOTATION['annotations']) + 9000000000\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"We could not find an annotation file. If the directory is empty continue, otherwise it is not reccomended to proceed\")\n",
    "                if not self.__proceed():\n",
    "                    return\n",
    "                self.SET_ALRDY_EXISTS = False\n",
    "\n",
    "        # ..or create new None\n",
    "        if not self.SET_ALRDY_EXISTS:\n",
    "            today = str(datetime.datetime.utcnow())\n",
    "            categories = []\n",
    "            for idx, label in enumerate(self.CLASSES):\n",
    "                categories.append({'supercategory': 'object', 'id': idx, 'name': label})\n",
    "            self.ANNOTATION = {\n",
    "                'info': {'year': 2020,'version': None,'description': 'Pollenforager Detection','contributor': 'Mara Kortenkamp, Tim Feige','url': 'https://github.com/marakortenkamp/pollen-detection','date_created': today},\n",
    "                'images': [],\n",
    "                'annotations': [],\n",
    "                'licenses': {'id': None,'name': None,'url': None,},\n",
    "                'category': categories\n",
    "                }\n",
    "            self.TOTAL_IMAGES, self.TOTAL_ANNOTATIONS = 0, 9000000000\n",
    "\n",
    "        # find image-annotation pairs\n",
    "        # go find an image:\n",
    "        for root, folders, files in os.walk(self.RAW_DATA_PATH):\n",
    "            for folder in folders:\n",
    "                if folder == 'img':\n",
    "                    for img_root, img_folder, img_files in os.walk(os.path.join(root, folder)):\n",
    "                        for img_file in img_files:\n",
    "                            # go find its realted annotation file:\n",
    "                            found_ann = False\n",
    "                            for ann_root, ann_folder, ann_files in os.walk(os.path.join(root, 'ann')):\n",
    "                                if found_ann:\n",
    "                                    break\n",
    "                                for ann_file in ann_files:\n",
    "                                    # found a pair!\n",
    "                                    if img_file in ann_file:\n",
    "                                        # create dictionary for current img with all classes as keys and coords of objects in current image as values:\n",
    "                                        # class_coords { 'class1' : [(x,y),..], 'class2' : [(x,y), (x,y), ..], .. }\n",
    "                                        class_coords = { i : [] for i in self.CLASSES}\n",
    "                                        with open(os.path.join(ann_root, ann_file)) as ann_json:\n",
    "                                            ann_data = json.load(ann_json)\n",
    "                                        if len(ann_data['objects']):\n",
    "                                            for obj in ann_data['objects']:\n",
    "                                                class_coords[obj['classTitle']].append(obj['points']['exterior'][0])\n",
    "                                        # crop image around objects, get annotation information of each object\n",
    "                                        img_path = os.path.join(img_root, img_file)\n",
    "                                        self.__process_image(img_path, class_coords)\n",
    "                                        # to prevent unnecessary looping:\n",
    "                                        found_ann = True\n",
    "                                        break\n",
    "        # save annotation file\n",
    "        with open(os.path.join(self.DATASET_PATH, 'annotation.json'), 'w') as fp:\n",
    "            json.dump(self.ANNOTATION, fp)\n",
    "\n",
    "        print(\"Data created.\")\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------#\n",
    "    ##### Helper functions #####\n",
    "    #--------------------------#\n",
    "\n",
    "    def __getClasses(self,dir):\n",
    "        \"\"\"Return list of classes found in annotation file(s)\n",
    "\n",
    "        Keyword arguments:\n",
    "        dir        -- path to raw data\n",
    "\n",
    "        Note: classnames must be stored in some.json: {.., 'objects': [{.., 'classTitle':<classname>, ..}, ..]}\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        for root, dicts, files in os.walk(dir):\n",
    "            for file in files:\n",
    "                if (file[-5:] == '.json'):\n",
    "                    with open(os.path.join(root, file)) as json_file:\n",
    "                        data = json.load(json_file)\n",
    "                        try:\n",
    "                            if len(data['objects']):\n",
    "                                for object in data['objects']:\n",
    "                                    if object['classTitle'] == 'est':\n",
    "                                        print(\"est location: \", root, file)\n",
    "                                    if not object['classTitle'] in classes:\n",
    "                                        classes.append(object['classTitle'])\n",
    "                        except:\n",
    "                            pass\n",
    "        return classes\n",
    "\n",
    "\n",
    "\n",
    "    def __proceed(self):\n",
    "        while True:\n",
    "            proceed = input(\"Do you want to continue?(Y/N): \")\n",
    "            if proceed.upper() == 'Y':\n",
    "                return True\n",
    "            elif proceed.upper() == 'N':\n",
    "                return False\n",
    "\n",
    "\n",
    "\n",
    "    def __process_image(self, filepath, coord_dic):\n",
    "        \"\"\"Creates cropped images for each coord in raw image and updates the COCO file.\n",
    "        If requested augmented duplicates of those images are made, annotations will be provided.\n",
    "        If requested, an image around random coords without annotation is created for a fake class.\n",
    "        \"\"\"\n",
    "\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        if self.COUNTERCLASS:\n",
    "            coord_dic = self.__add_rnd_coords(coord_dic)\n",
    "\n",
    "        for label in self.CLASSES:\n",
    "            for coord in coord_dic[label]:\n",
    "                self.__extract_object(img, coord, label)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __add_rnd_coords(self, coord_dic):\n",
    "        \"\"\"Generates a set of random coordinates. We assume that each class in self.CLASSES is about the same size,\n",
    "        thus we create as many random coords as self.CLASSES[0] has in the current 'coord_dic'.\n",
    "        Images cropped around random coordinates will not overlap with labeled objects.\n",
    "\n",
    "        Note: This is optimised for the BeeProject, as it wont crop in specified areas\n",
    "        Valid areas: [200,200]-[2350,3800] , [2340,50]-[2640,625] , [2330,3500]-[2640,3990]\n",
    "\n",
    "        Note 2: Superviselys points are in (x,y) format, cv2 opens files in (y,x) format, we will save coords in (x,y) for persistence\n",
    "        \"\"\"\n",
    "\n",
    "        if self.CLASSES[0] != 'Fake':\n",
    "            rnd_size = len(coord_dic[self.CLASSES[0]])\n",
    "        else:\n",
    "            rnd_size = len(coord_dic[self.CLASSES[1]])\n",
    "        \n",
    "        rnd_size = len(coord_dic[self.CLASSES[idx]])\n",
    "\n",
    "        offset = self.OBJECT_SIZE // 2\n",
    "\n",
    "        for i in range(rnd_size):\n",
    "            too_close = True\n",
    "            while too_close:\n",
    "                # generate rndm coord\n",
    "                rnd_x = random.randint(50+offset,3990-offset)\n",
    "                if (rnd_x < 200) :\n",
    "                    rnd_y = random.randint(2330+offset,2640-offset)\n",
    "                elif (rnd_x <= 625) :\n",
    "                     rnd_y = random.randint(200+offset,2640-offset)\n",
    "                elif (rnd_x < 3500) :\n",
    "                    rnd_y = random.randint(200+offset,2350-offset)\n",
    "                elif (rnd_x <= 3800) :\n",
    "                    rnd_y = random.randint(200+offset,2640-offset)\n",
    "                elif (rnd_x <= 3990) :\n",
    "                    rnd_y = random.randint(2330+offset,2640-offset)\n",
    "                rnd_coord = [rnd_x, rnd_y]\n",
    "                # interfere with any object?\n",
    "                next_try = False\n",
    "                for label in self.CLASSES:\n",
    "                    if next_try:\n",
    "                        break\n",
    "                    for coord in coord_dic[label]:\n",
    "                        threshold = self.OBJECT_SIZE//2\n",
    "                        too_close = self.__eukl_dist(coord, rnd_coord, threshold)\n",
    "                        if too_close:\n",
    "                            next_try = True\n",
    "                            break\n",
    "\n",
    "                if not too_close:\n",
    "                    coord_dic['Fake'].append(rnd_coord)\n",
    "\n",
    "        return coord_dic\n",
    "\n",
    "\n",
    "\n",
    "    def __eukl_dist(self, p, q, threshold):\n",
    "        \"\"\"Returns boolean whether distance is smaller than threshold\"\"\"\n",
    "        d = math.sqrt(((p[0]-q[0])**2)+((p[1]-q[1])**2))\n",
    "        return (d <= threshold)\n",
    "\n",
    "\n",
    "\n",
    "    def __extract_object(self, img, coord, label):\n",
    "        \"\"\"Crop image around coord,\n",
    "           augment cropped image,\n",
    "           save image in class directorys,\n",
    "           update self.ANNOTATION for none-Fake images\n",
    "\n",
    "           Note: cv2-Images have format [y,x], our coords have format [x,y]\n",
    "           Note2: cv2 Image arrays start at 0, our coord arrays start at 1\n",
    "        \"\"\"\n",
    "\n",
    "        final_images = []\n",
    "        if self.AUGMENTATION:\n",
    "            #for augmentation we crop a larger image, so we can rotate more easily\n",
    "            object_size = self.OBJECT_SIZE+(self.OBJECT_SIZE//2)\n",
    "        else:\n",
    "            object_size = self.OBJECT_SIZE\n",
    "        raw_img_size = img.shape        # raw_img_size has format (y,x)\n",
    "        even = object_size%2\n",
    "        y = coord[1]-1\n",
    "        x = coord[0]-1\n",
    "\n",
    "        start_y = y - (object_size//2)\n",
    "        if ( start_y < 0 ):\n",
    "            start_y = 0\n",
    "            border_top = (object_size//2)-y\n",
    "        else:\n",
    "            border_top = 0\n",
    "\n",
    "        end_y = start_y + object_size - border_top\n",
    "        if end_y > raw_img_size[0]:\n",
    "            border_bottom = raw_img_size[0]-end_y\n",
    "            end_y = raw_img_size[0]\n",
    "        else:\n",
    "            border_bottom = 0\n",
    "\n",
    "        start_x = x - (object_size//2)\n",
    "        if ( start_x < 0 ):\n",
    "            start_x = 0\n",
    "            border_left = (object_size//2)-x\n",
    "        else:\n",
    "            border_left = 0\n",
    "\n",
    "        end_x = start_x + object_size - border_left\n",
    "        if end_x > raw_img_size[1]:\n",
    "            border_right = raw_img_size[1]-end_x\n",
    "            end_x = raw_img_size[1]\n",
    "        else:\n",
    "            border_right = 0\n",
    "\n",
    "        object = img[start_y:end_y, start_x:end_x]\n",
    "        object = cv2.copyMakeBorder(object, border_top, border_bottom, border_left, border_right, cv2.BORDER_REPLICATE)\n",
    "\n",
    "        # create augmented versions of current cropped image\n",
    "        # after augmentation, crop to size OBJECT_SIZE\n",
    "        # resize_to is applied in the end\n",
    "        if self.AUGMENTATION:\n",
    "            final_images = self.__augment_img(object)\n",
    "        else:\n",
    "            if self.FINAL_SIZE:\n",
    "                object = cv2.resize(object, (self.FINAL_SIZE,self.FINAL_SIZE))\n",
    "            final_images.append(object)\n",
    "\n",
    "        # safe images to storage and update annotation\n",
    "        for final_image in final_images:\n",
    "            self.TOTAL_IMAGES += 1\n",
    "            filename = str(self.TOTAL_IMAGES) + '.png'\n",
    "\n",
    "            if label != 'Fake':\n",
    "                self.TOTAL_ANNOTATIONS += 1\n",
    "                timestmp = str(datetime.datetime.utcnow())\n",
    "                anno_img = {'id':self.TOTAL_IMAGES,'width':self.OBJECT_SIZE,'height':self.OBJECT_SIZE,'file_name':filename,'license':None,'flickr_url':None,'coco_url':None,'date_captured':timestmp}\n",
    "                anno_anno = {'id':self.TOTAL_ANNOTATIONS,'image_id':self.TOTAL_IMAGES,'category_id':label,'segmentation':[],'area':final_image.size,'bbox':[],'iscrowd':0}\n",
    "                self.ANNOTATION['images'].append(anno_img)\n",
    "                self.ANNOTATION['annotations'].append(anno_anno)\n",
    "            savefileat = os.path.join(self.DATASET_PATH, label, filename)\n",
    "            cv2.imwrite(savefileat, final_image)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # create augmented versions of current cropped image\n",
    "    # after augmentation, crop to size OBJECT_SIZE\n",
    "    # resize_to is applied in the end\n",
    "    def __augment_img(self, input_img):\n",
    "        images = []\n",
    "        final_images =[]\n",
    "\n",
    "        # apply on input_img\n",
    "        fliplr = iaa.Fliplr(1)\n",
    "        flipud = iaa.Flipud(1)\n",
    "\n",
    "        # apply on input_img and each flip\n",
    "        rotate45 = iaa.Affine(rotate=(45))\n",
    "        rotate90 = iaa.Rot90(1)\n",
    "        rotate135 = iaa.Affine(rotate=(135))\n",
    "        rotate180 = iaa.Rot90(2)\n",
    "        rotate225 = iaa.Affine(rotate=(-135))\n",
    "        rotate270 = iaa.Rot90(3)\n",
    "        rotate315 = iaa.Affine(rotate=(-45))\n",
    "\n",
    "        # apply on input_img, each flip and each rotation\n",
    "        gauss = iaa.AdditiveGaussianNoise(scale=0.15*255)\n",
    "        poison = iaa.AdditivePoissonNoise(40)\n",
    "        brighter = iaa.Multiply(1.5)\n",
    "        darker = iaa.Multiply(0.5)\n",
    "        gaussblur = iaa.GaussianBlur(sigma=(1))\n",
    "        motionblur = iaa.MotionBlur(k=3)\n",
    "\n",
    "        final_images.append(input_img)\n",
    "        for img in final_images:\n",
    "            img_fliplr = fliplr(image=img)\n",
    "            img_flipud = flipud(image=img)\n",
    "            img_fliphv = fliplr(image=img)\n",
    "            img_fliphv = flipud(image=img_fliphv)\n",
    "            images.append(img)\n",
    "            images.append(img_fliplr)\n",
    "            images.append(img_flipud)\n",
    "            images.append(img_fliphv)\n",
    "        final_images = []\n",
    "        for img in images:\n",
    "            img_rot45 = rotate45(image=img)\n",
    "            img_rot90 = rotate90(image=img)\n",
    "            img_rot135 = rotate135(image=img)\n",
    "            img_rot180 = rotate180(image=img)\n",
    "            img_rot225 = rotate225(image=img)\n",
    "            img_rot270 = rotate270(image=img)\n",
    "            img_rot315 = rotate315(image=img)\n",
    "            final_images.append(img)\n",
    "            final_images.append(img_rot45)\n",
    "            final_images.append(img_rot90)\n",
    "            final_images.append(img_rot135)\n",
    "            final_images.append(img_rot180)\n",
    "            final_images.append(img_rot225)\n",
    "            final_images.append(img_rot270)\n",
    "            final_images.append(img_rot315)\n",
    "        images = []\n",
    "        for img in final_images:\n",
    "            img_gauss = gauss(image=img)\n",
    "            img_poison = poison(image=img)\n",
    "            img_bright = brighter(image=img)\n",
    "            img_dark = darker(image=img)\n",
    "            img_gblur = gaussblur(image=img)\n",
    "            img_mblur = motionblur(image=img)\n",
    "            images.append(img)\n",
    "            images.append(img_gauss)\n",
    "            images.append(img_poison)\n",
    "            images.append(img_bright)\n",
    "            images.append(img_dark)\n",
    "            images.append(img_gblur)\n",
    "            images.append(img_mblur)\n",
    "\n",
    "        top_crop = math.ceil((self.OBJECT_SIZE//2)/2)\n",
    "        right_crop = (self.OBJECT_SIZE//2)//2\n",
    "        bottom_crop = (self.OBJECT_SIZE//2)//2\n",
    "        left_crop = math.ceil((self.OBJECT_SIZE//2)/2)\n",
    "        img_size = input_img.shape\n",
    "\n",
    "        final_images = []\n",
    "        for img in images:\n",
    "            img = img[top_crop:(img_size[0]-bottom_crop), left_crop:(img_size[1]-right_crop)]\n",
    "            final_images.append(img)\n",
    "\n",
    "        if self.FINAL_SIZE:\n",
    "            for img in final_images:\n",
    "                img = cv2.resize(img, (self.FINAL_SIZE,self.FINAL_SIZE))\n",
    "\n",
    "        return final_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directories..\n",
      "Directories created.\n",
      "Creating Data..\n",
      "Data created.\n"
     ]
    }
   ],
   "source": [
    "BeeData = SVObjectExtractor(raw_path=\"/home/tkf/Desktop/Uni/SWP/pollen-detection/raw_data\", dataset_name=\"PollenData\", object_size=32, augmentation=True, counterclass=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

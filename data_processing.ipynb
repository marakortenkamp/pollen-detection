{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import errno\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "# !pip3 install git+https://github.com/aleju/imgaug\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SVObjectExtractor:\n",
    "    \"\"\"Prepare unprocessed Data downlaoded from supervisely\n",
    "    \n",
    "        SuperVisely Object Extractor\n",
    "        For each raw image containing multiple objects markted by Point, Bounding Box or Polygon, we create images cropped to given 'size', with each object centered.\n",
    "        We create directories for each class containing those cropped images and a single annotation file in COCO format.\n",
    "        You can create a 'counterclass', with random images cropped from raw images (it is guaranteed those images wont overlap with objects of other classes).\n",
    "        You can chose to use 'augmentation' to expand the size of the dataset.\n",
    "\n",
    "        Final structure:\n",
    "\n",
    "        root/\n",
    "         ├───raw_data_directory/\n",
    "         │\n",
    "         └───Datasets/\n",
    "                ├───<DatasetName>\n",
    "                │       ├───annotation.json\n",
    "               ...      ├───labelstrength.pickle\n",
    "                        ├───Class1/\n",
    "                        ├───Class2/\n",
    "                       ...\n",
    "                        └───ClassN/\n",
    "                        └───Fake/ (created if 'counterclass' is True)\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, raw_path, target_path=None, dataset_name=None, object_size=None, resize_to=None, augmentation=False, counterclass=False):\n",
    "        \"\"\"Args:\n",
    "            raw_path        --      path to raw data\n",
    "            target_path     --      path to save dataset at. If an existing dataset shall be expanded, give the location of the <Datasetname> directory.\n",
    "            dataset_name    --      name of the dataset\n",
    "            object_size     --      size of objects in images (single integer. only sqaures generated)\n",
    "            resize_to       --      resize cropped images to resize_to (single integer. only squares generated)\n",
    "            augmentation    --      if True, dataset will 5be expanded by augmentations of the cropped images (default False)\n",
    "            counterclass    --      if True, a 'Fake' class with random images of size 'size' will be created\n",
    "\n",
    "        Attributes:\n",
    "            raw_data_path   --      path to raw data\n",
    "            dataset_path    --      path to dataset directory\n",
    "            dataset_name    --      name of the set to be created\n",
    "            object_size     --      size of cropped images around segmentation\n",
    "            final_size      --      final size of cropped images\n",
    "            augmentation    --      boolean whether augmentations of cropped images shall be created or not\n",
    "            counterclass    --      boolean, whether a fake class should be created or not\n",
    "            classes         --      list of classes found\n",
    "            annotation      --      contains json in coco format for dataset about to be made\n",
    "            total_images    --      number of all objects found, will be used as image-name and image-id\n",
    "            total_annotations   --  number of annotations made for 'self.annotation' json. Will start with 9000000000 to seperate from image-id\n",
    "            DATA_CREATED    --      boolen whether create_data was alrdy called or not\n",
    "            DIRS_MADE       --      boolen whether directories are created or not\n",
    "            SET_ALRDY_EXISTS      --  boolean whether the Dataset directories already exist or not\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isdir(raw_path):\n",
    "            self.raw_data_path = raw_path\n",
    "            invalid_path = False\n",
    "            abort = False\n",
    "        else:\n",
    "            print(\"Error. Given path to raw data does not exist.\")\n",
    "            raise ValueError\n",
    "            invalid_path = True\n",
    "        if not invalid_path:\n",
    "            self.dataset_name = dataset_name\n",
    "            self.DIRS_MADE = False\n",
    "            self.SET_ALRDY_EXISTS = False\n",
    "\n",
    "            # some cases to check, to get the right path\n",
    "            setname_len = len(self.dataset_name)\n",
    "            if target_path:\n",
    "                self.dataset_path = target_path\n",
    "            else:\n",
    "                self.dataset_path = os.getcwd()\n",
    "\n",
    "            if self.dataset_name in self.dataset_path[-(setname_len+1):]:\n",
    "                self.SET_ALRDY_EXISTS = True\n",
    "\n",
    "            elif 'Datasets' in self.dataset_path[-(len('Datasets')+1):]:\n",
    "                if os.path.isdir(os.path.join(self.dataset_path,self.dataset_name)):\n",
    "                    print(\"A directory for given datasetname already exists. The set will be expanded.\")\n",
    "                    if not self.__proceed(\"Do you want to expand the existing set?\"):\n",
    "                        raise ValueError\n",
    "                        abort = True\n",
    "                    else:\n",
    "                        self.dataset_path = os.path.join(self.dataset_path,self.dataset_name)\n",
    "                        self.SET_ALRDY_EXISTS = True\n",
    "                else:\n",
    "                    self.dataset_path = os.path.join(self.dataset_path, self.dataset_name)\n",
    "            else:\n",
    "                if os.path.isdir(os.path.join(self.dataset_path, 'Datasets', self.dataset_name)):\n",
    "                    print(\"A path to given datasetname already exists. The set will be expanded.\")\n",
    "                    if not self.__proceed(\"Do you want to expand the existing set?\"):\n",
    "                        raise ValueError\n",
    "                        abort = True\n",
    "                    else:\n",
    "                        self.SET_ALRDY_EXISTS = True\n",
    "                self.dataset_path = os.path.join(self.dataset_path, 'Datasets', self.dataset_name)\n",
    "\n",
    "            if not abort:\n",
    "                self.object_size = object_size\n",
    "                self.final_size = resize_to\n",
    "                self.augmentation = augmentation\n",
    "                self.counterclass = counterclass\n",
    "                self.classes = self.__getClasses(raw_path)\n",
    "                if self.counterclass:\n",
    "                    self.classes.append('Fake')\n",
    "                self.DATA_CREATED = False\n",
    "                self.annotation = None\n",
    "                self.total_images = None\n",
    "                self.total_annotations = None\n",
    "\n",
    "                dir_ready = self._make_directories()\n",
    "                if dir_ready:\n",
    "                    self._create_data()\n",
    "\n",
    "\n",
    "\n",
    "    def _make_directories(self):\n",
    "        \"\"\"Create a direcotry for each class in self.classes\"\"\"\n",
    "\n",
    "        print(\"Create directories..\")\n",
    "        if self.SET_ALRDY_EXISTS:\n",
    "            if self.DATA_CREATED:\n",
    "                print(\"Everything is already done.\")\n",
    "                return False\n",
    "\n",
    "        elif self.DIRS_MADE:\n",
    "            print(\"Error. Directories were already created, yet could not be found.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            for label in self.classes:\n",
    "                os.makedirs(os.path.join(self.dataset_path, label))\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                print(e)\n",
    "                raise\n",
    "                return False\n",
    "            pass\n",
    "        self.DIRS_MADE = True\n",
    "        print(\"Directories created.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Creates annotationfile and cropped images of raw image.\n",
    "    \"\"\"\n",
    "    def _create_data(self):\n",
    "        \"\"\"Creates annotationfile and cropped images of raw image.\"\"\"\n",
    "        print(\"Process Data...\")\n",
    "        # Annotation in COCO format\n",
    "        # either load existing annotation file..\n",
    "        if self.SET_ALRDY_EXISTS:\n",
    "            print(\"Searching for existing annotation file..\")\n",
    "            try:\n",
    "                with open(os.path.join(self.dataset_path, 'annotation.json')) as annotation_file:\n",
    "                    self.annotation = json.load(annotation_file)\n",
    "                print(\"Loading file succeeded.\")\n",
    "                self.total_images = len(self.annotation['images'])*2\n",
    "                self.total_annotations = len(self.annotation['annotations']) + 9000000000\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"We could not find an annotation file. If the directory is empty continue, otherwise it is not reccomended to proceed\")\n",
    "                if not self.__proceed():\n",
    "                    return\n",
    "                self.SET_ALRDY_EXISTS = False\n",
    "\n",
    "        # ..or create new None\n",
    "        if not self.SET_ALRDY_EXISTS:\n",
    "            today = str(datetime.datetime.utcnow())\n",
    "            categories = []\n",
    "            for idx, label in enumerate(self.classes):\n",
    "                categories.append({'supercategory': 'object', 'id': idx, 'name': label})\n",
    "            self.annotation = {\n",
    "                'info': {'year': 2020,'version': None,'description': 'Pollenforager Detection','contributor': 'Mara Kortenkamp, Tim Feige','url': 'https://github.com/marakortenkamp/pollen-detection','date_created': today},\n",
    "                'images': [],\n",
    "                'annotations': [],\n",
    "                'licenses': {'id': None,'name': None,'url': None,},\n",
    "                'category': categories\n",
    "                }\n",
    "            self.total_images, self.total_annotations = 0, 9000000000\n",
    "\n",
    "        # find image-annotation pairs\n",
    "        # go find an image:\n",
    "        for root, folders, files in os.walk(self.raw_data_path):\n",
    "            for folder in folders:\n",
    "                if folder == 'img':\n",
    "                    for img_root, img_folder, img_files in os.walk(os.path.join(root, folder)):\n",
    "                        for img_file in img_files:\n",
    "                            # go find its realted annotation file:\n",
    "                            found_ann = False\n",
    "                            for ann_root, ann_folder, ann_files in os.walk(os.path.join(root, 'ann')):\n",
    "                                if found_ann:\n",
    "                                    break\n",
    "                                for ann_file in ann_files:\n",
    "                                    # found a pair!\n",
    "                                    if img_file in ann_file:\n",
    "                                        # create dictionary for current img with all classes as keys and coords of objects in current image as values:\n",
    "                                        # class_coords { 'class1' : [(x,y),..], 'class2' : [(x,y), (x,y), ..], .. }\n",
    "                                        class_coords = { i : [] for i in self.classes}\n",
    "                                        with open(os.path.join(ann_root, ann_file)) as ann_json:\n",
    "                                            ann_data = json.load(ann_json)\n",
    "                                        if len(ann_data['objects']):\n",
    "                                            for obj in ann_data['objects']:\n",
    "                                                class_coords[obj['classTitle']].append(obj['points']['exterior'][0])\n",
    "                                        # crop image around objects, get annotation information of each object\n",
    "                                        img_path = os.path.join(img_root, img_file)\n",
    "                                        self.__process_image(img_path, class_coords)\n",
    "                                        # to prevent unnecessary looping:\n",
    "                                        found_ann = True\n",
    "                                        break\n",
    "        # save annotation file\n",
    "        with open(os.path.join(self.dataset_path, 'annotation.json'), 'w') as fp:\n",
    "            json.dump(self.annotation, fp)\n",
    "\n",
    "        print(\"Data created.\")\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------#\n",
    "    ##### Helper functions #####\n",
    "    #--------------------------#\n",
    "\n",
    "    def __getClasses(self, dir):\n",
    "        \"\"\"Return list of classes found in annotation file(s)\n",
    "\n",
    "        Args:\n",
    "            dir        -- path to raw data\n",
    "\n",
    "        Note: classnames must be stored in some.json: {.., 'objects': [{.., 'classTitle':<classname>, ..}, ..]}\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        for root, dicts, files in os.walk(dir):\n",
    "            for file in files:\n",
    "                if (file[-5:] == '.json'):\n",
    "                    with open(os.path.join(root, file)) as json_file:\n",
    "                        data = json.load(json_file)\n",
    "                        try:\n",
    "                            if len(data['objects']):\n",
    "                                for object in data['objects']:\n",
    "                                    if object['classTitle'] == 'est':\n",
    "                                        print(\"est location: \", root, file)\n",
    "                                    if not object['classTitle'] in classes:\n",
    "                                        classes.append(object['classTitle'])\n",
    "                        except:\n",
    "                            pass\n",
    "        return classes\n",
    "\n",
    "\n",
    "\n",
    "    def __proceed(self, string=None):\n",
    "        \"\"\"Ask for userinput\"\"\"\n",
    "        while True:\n",
    "            if string:\n",
    "                proceed = input(string + \" (Y/N): \")\n",
    "            else:\n",
    "                proceed = input(\"Do you want to continue?(Y/N): \")\n",
    "            if proceed.upper() == 'Y':\n",
    "                return True\n",
    "            elif proceed.upper() == 'N':\n",
    "                return False\n",
    "\n",
    "\n",
    "\n",
    "    def __process_image(self, filepath, coord_dic):\n",
    "        \"\"\"Creates cropped images for each coord in raw image and updates the COCO file.\n",
    "        If requested augmented duplicates of those images are made, annotations will be provided.\n",
    "        If requested, an image around random coords without annotation is created for a fake class.\n",
    "        \"\"\"\n",
    "\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        if self.counterclass:\n",
    "            coord_dic = self.__add_rnd_coords(coord_dic)\n",
    "\n",
    "        for label in self.classes:\n",
    "            for coord in coord_dic[label]:\n",
    "                self.__extract_object(img, coord, label)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __add_rnd_coords(self, coord_dic):\n",
    "        \"\"\"Generates a set of random coordinates. We assume that each class in self.classes is about the same size,\n",
    "        thus we create as many random coords as a fix class from self.classes has in the current 'coord_dic'.\n",
    "        We assure that images cropped around random coordinates will not overlap with labeled objects.\n",
    "\n",
    "        Note: Superviselys points are in (x,y) format, cv2 opens files in (y,x) format, we will save coords in (x,y) for persistence\n",
    "        \"\"\"\n",
    "\n",
    "        if self.classes[0] != 'Fake':\n",
    "            rnd_size = len(coord_dic[self.classes[0]])\n",
    "        else:\n",
    "            rnd_size = len(coord_dic[self.classes[1]])\n",
    "\n",
    "        offset = self.object_size // 2\n",
    "\n",
    "        for i in range(9*rnd_size):\n",
    "            too_close = True\n",
    "            while too_close:\n",
    "                # generate rndm coord\n",
    "                rnd_x = random.randint(0+offset,3990-offset)\n",
    "                rnd_y = random.randint(0+offset,2999-offset)\n",
    "                rnd_coord = [rnd_x, rnd_y]\n",
    "                # interfere with any object?\n",
    "                next_try = False\n",
    "                for label in self.classes:\n",
    "                    if next_try:\n",
    "                        break\n",
    "                    for coord in coord_dic[label]:\n",
    "                        threshold = self.object_size//2\n",
    "                        too_close = self.__eukl_dist(coord, rnd_coord, threshold)\n",
    "                        if too_close:\n",
    "                            next_try = True\n",
    "                            break\n",
    "\n",
    "                if not too_close:\n",
    "                    coord_dic['Fake'].append(rnd_coord)\n",
    "\n",
    "        return coord_dic\n",
    "\n",
    "\n",
    "\n",
    "    def __eukl_dist(self, p, q, threshold):\n",
    "        \"\"\"Returns boolean whether distance is smaller than threshold\"\"\"\n",
    "        d = math.sqrt(((p[0]-q[0])**2)+((p[1]-q[1])**2))\n",
    "        return (d <= threshold)\n",
    "\n",
    "\n",
    "\n",
    "    def __extract_object(self, img, coord, label):\n",
    "        \"\"\"Crop image around coord,\n",
    "           augment cropped image,\n",
    "           save image in class directorys,\n",
    "           update self.annotation for none-Fake images\n",
    "\n",
    "           Note: cv2-Images have format [y,x], our coords have format [x,y]\n",
    "        \"\"\"\n",
    "\n",
    "        final_images = []\n",
    "        if self.augmentation:\n",
    "            #for augmentation we crop a larger image, so we can rotate more easily\n",
    "            object_size = self.object_size+(self.object_size//2)\n",
    "        else:\n",
    "            object_size = self.object_size\n",
    "        raw_img_size = img.shape        # raw_img_size has format (y,x)\n",
    "        y = coord[1]\n",
    "        if y == raw_img_size[0]:\n",
    "            y == raw_img_size[0]-1\n",
    "        x = coord[0]\n",
    "        if x == raw_img_size[1]:\n",
    "            x == raw_img_size[1]-1 \n",
    "\n",
    "        start_y = y - (object_size//2)\n",
    "        if ( start_y < 0 ):\n",
    "            start_y = 0\n",
    "            border_top = (object_size//2)-y\n",
    "        else:\n",
    "            border_top = 0\n",
    "\n",
    "        end_y = start_y + object_size - border_top\n",
    "        if end_y > raw_img_size[0]:\n",
    "            border_bottom = end_y-raw_img_size[0]\n",
    "            end_y = raw_img_size[0]\n",
    "        else:\n",
    "            border_bottom = 0\n",
    "\n",
    "        start_x = x - (object_size//2)\n",
    "        if ( start_x < 0 ):\n",
    "            start_x = 0\n",
    "            border_left = (object_size//2)-x\n",
    "        else:\n",
    "            border_left = 0\n",
    "\n",
    "        end_x = start_x + object_size - border_left\n",
    "        if end_x > raw_img_size[1]:\n",
    "            border_right = end_x-raw_img_size[1]\n",
    "            end_x = raw_img_size[1]\n",
    "        else:\n",
    "            border_right = 0\n",
    "\n",
    "        object = img[start_y:end_y, start_x:end_x]\n",
    "        object = cv2.copyMakeBorder(object, border_top, border_bottom, border_left, border_right, cv2.BORDER_REPLICATE)\n",
    "\n",
    "        if self.augmentation:\n",
    "            final_images = self.__augment_img(object)\n",
    "        else:\n",
    "            if self.final_size:\n",
    "                object = cv2.resize(object, (self.final_size,self.final_size))\n",
    "            final_images.append(object)\n",
    "\n",
    "        # safe images to storage and update annotation\n",
    "        for final_image in final_images:\n",
    "            self.total_images += 1\n",
    "            filename = str(self.total_images) + '.png'\n",
    "\n",
    "            if label != 'Fake':\n",
    "                self.total_annotations += 1\n",
    "                timestmp = str(datetime.datetime.utcnow())\n",
    "                anno_img = {'id':self.total_images,'width':self.object_size,'height':self.object_size,'file_name':filename,'license':None,'flickr_url':None,'coco_url':None,'date_captured':timestmp}\n",
    "                anno_anno = {'id':self.total_annotations,'image_id':self.total_images,'category_id':label,'segmentation':[],'area':final_image.size,'bbox':[],'iscrowd':0}\n",
    "                self.annotation['images'].append(anno_img)\n",
    "                self.annotation['annotations'].append(anno_anno)\n",
    "            savefileat = os.path.join(self.dataset_path, label, filename)\n",
    "            cv2.imwrite(savefileat, final_image)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"Create altered versions of given image.\n",
    "        Return set of alterations.\n",
    "    \"\"\"\n",
    "    def __augment_img(self, input_img):\n",
    "        \"\"\"Create set of alterations of given image.\"\"\"\n",
    "        images = []\n",
    "        final_images =[]\n",
    "\n",
    "        # apply on input_img\n",
    "        fliplr = iaa.Fliplr(1)\n",
    "        flipud = iaa.Flipud(1)\n",
    "\n",
    "        # apply on input_img and each flip\n",
    "        rotate60 = iaa.Affine(rotate=(60))\n",
    "        rotate120 = iaa.Affine(rotate=(120))\n",
    "        rotate180 = iaa.Rot90(2)\n",
    "        rotate240 = iaa.Affine(rotate=(-120))\n",
    "        rotate300 = iaa.Affine(rotate=(-60))\n",
    "\n",
    "        # apply on input_img, each flip and each rotation\n",
    "        gauss = iaa.AdditiveGaussianNoise(scale=0.04*255)\n",
    "        brighter = iaa.Multiply(1.5)\n",
    "        darker = iaa.Multiply(0.5)\n",
    "        gaussblur = iaa.GaussianBlur(sigma=(1))\n",
    "\n",
    "        final_images.append(input_img)\n",
    "        for img in final_images:\n",
    "            img_fliplr = fliplr(image=img)\n",
    "            img_flipud = flipud(image=img)\n",
    "            images.append(img)\n",
    "            images.append(img_fliplr)\n",
    "            images.append(img_flipud)\n",
    "        final_images = []\n",
    "        for img in images:\n",
    "            img_rot60 = rotate60(image=img)\n",
    "            img_rot120 = rotate120(image=img)\n",
    "            img_rot180 = rotate180(image=img)\n",
    "            img_rot240 = rotate240(image=img)\n",
    "            img_rot300 = rotate300(image=img)\n",
    "            final_images.append(img)\n",
    "            final_images.append(img_rot60)\n",
    "            final_images.append(img_rot120)\n",
    "            final_images.append(img_rot180)\n",
    "            final_images.append(img_rot240)\n",
    "            final_images.append(img_rot300)\n",
    "        images = []\n",
    "        for img in final_images:\n",
    "            img_gauss = gauss(image=img)\n",
    "            img_bright = brighter(image=img)\n",
    "            img_dark = darker(image=img)\n",
    "            img_gblur = gaussblur(image=img)\n",
    "            images.append(img)\n",
    "            images.append(img_gauss)\n",
    "            images.append(img_bright)\n",
    "            images.append(img_dark)\n",
    "            images.append(img_gblur)\n",
    "\n",
    "        top_crop = math.ceil((self.object_size//2)/2)\n",
    "        right_crop = (self.object_size//2)//2\n",
    "        bottom_crop = (self.object_size//2)//2\n",
    "        left_crop = math.ceil((self.object_size//2)/2)\n",
    "        img_size = input_img.shape\n",
    "\n",
    "        final_images = []\n",
    "        for img in images:\n",
    "            img = img[top_crop:(img_size[0]-bottom_crop), left_crop:(img_size[1]-right_crop)]\n",
    "            final_images.append(img)\n",
    "\n",
    "        if self.final_size:\n",
    "            for img in final_images:\n",
    "                img = cv2.resize(img, (self.final_size,self.final_size))\n",
    "\n",
    "        return final_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A path to given datasetname already exists. The set will be expanded.\n",
      "Do you want to expand the existing set? (Y/N): N\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2fa26a13e239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBeeData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVObjectExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/tkf/Desktop/Uni/SWP/pollen-detection/raw_data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/tkf/Desktop/Uni/SWP/pollen-detection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PollenData64\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounterclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a476eb938a30>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw_path, target_path, dataset_name, object_size, resize_to, augmentation, counterclass)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A path to given datasetname already exists. The set will be expanded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proceed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do you want to expand the existing set?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                         \u001b[0mabort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BeeData = SVObjectExtractor(raw_path=\"/home/tkf/Desktop/Uni/SWP/pollen-detection/raw_data\",target_path=\"/home/tkf/Desktop/Uni/SWP/pollen-detection\", dataset_name=\"PollenData64\", object_size=64, augmentation=True, counterclass=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
